# -*- coding: utf-8 -*-
"""Shopassist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aEiv37IrnN_WxM4K8vAtO_Kn12QKjdL_
"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from joblib import load

# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('averaged_perceptron_tagger')

data = pd.read_json('data/data.json')

data.shape

data.isnull().sum()

data.dropna()

"""Extract feature text from feature column"""

feature_text = []
for features in data['features']:
    feature_list = []
    for feature in features:
        for key, value in feature.items():
            feature_list.append(value.replace(" ", ""))
    feature_text.append(' '.join(feature_list))

""" Clean data"""

data['price'] = data['price'].astype(str)

data['text'] = data['title'] + ' ' + data['breadcrumbs'] + ' ' + \
    data['price'] + ' ' + data['brand'] + ' ' + pd.Series(feature_text)

data['text'].head(1)

data['text'] = data['text'].str.lower()

data['text'] = data['text'].str.split()

stopwords = stopwords.words('english')

data['text'] = data['text'].apply(
    lambda x: " ".join([w for w in x if w not in stopwords]))

data['text'].head(2)

final_df = data[['url', 'title', 'price', 'brand',
                 'images_list', 'features', 'text']].copy()

# POS Tagging and Word Filtering


def filter_important_words(text):
    # Tokenize and POS tag the text
    tokens = nltk.word_tokenize(text)
    tagged_tokens = nltk.pos_tag(tokens)

    # Define the allowed POS tags
    allowed_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD',
                    'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']

    # Filter out unimportant words based on their POS tags
    important_words = [
        word for word, tag in tagged_tokens if tag in allowed_tags and word.lower() not in stopwords]
    return ' '.join(important_words)


final_df['text'] = final_df['text'].apply(filter_important_words)

final_df['text'][0]

"""Create vectorizer"""

# vectorizer = TfidfVectorizer()
# X1 = vectorizer.fit_transform(data['text'])

vectorizer = load('logic/MLModels/recommenderV3.joblib')


def getTopSimilarProducts(query, numRecommendations=5):
    query = ' '.join([word.lower()
                      for word in word_tokenize(query) if word.isalpha()])
    query = ' '.join(
        [word for word in word_tokenize(query) if not word in stopwords])
    query_vector = vectorizer.transform([query])
    cosine_similarities = cosine_similarity(
        query_vector, vectorizer.transform(final_df['text'])).flatten()
    related_docs_indices = cosine_similarities.argsort()[
        :-numRecommendations-1:-1]
    return data.iloc[related_docs_indices].applymap(lambda x: float(x) if isinstance(x, float) else x)

# getTopSimilarProducts('I want black school shoes for boy', numRecommendations=3)
