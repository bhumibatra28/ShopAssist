# -*- coding: utf-8 -*-
"""Shopassist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aEiv37IrnN_WxM4K8vAtO_Kn12QKjdL_
"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

nltk.download('punkt')
nltk.download('stopwords')

data = pd.read_json('data/data.json')

# data.head()

data.shape

data.isnull().sum()

data.dropna()

# data.info()

"""Extract feature text from feature column"""

feature_text = []
for features in data['features']:
    feature_list = []
    for feature in features:
        for key, value in feature.items():
            feature_list.append(value.replace(" ",""))
    feature_text.append(' '.join(feature_list))

""" Clean data"""

data['price'] = data['price'].astype(str)

data['text'] = data['title'] + ' ' + data['breadcrumbs'] + ' ' + data['price']+ ' '+ data['brand'] + ' '+ pd.Series(feature_text)

data['text'].head(1)

data['text'] = data['text'].str.lower()

data['text'] = data['text'].str.split()

# data['text'] = data['text'].apply(lambda x: ' '.join([word.lower() for word in word_tokenize(x) if word.isalpha()]))
# stop_words = set(stopwords.words('english'))
# data['text'] = data['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if not word in stop_words]))

from nltk.corpus import stopwords
stopwords = stopwords.words('english')

data['text'] = data['text'].apply(lambda x: " ".join([w for w in x if w not in stopwords]))

data['text'].head(2)

final_df = data[['url','title','price','brand','images_list','features','text']].copy()

# POS Tagging and Word Filtering
nltk.download('averaged_perceptron_tagger')

from nltk import pos_tag
def filter_important_words(text):
    # Tokenize and POS tag the text
    tokens = nltk.word_tokenize(text)
    tagged_tokens = nltk.pos_tag(tokens)

    # Define the allowed POS tags
    allowed_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']

    # Filter out unimportant words based on their POS tags
    important_words = [word for word, tag in tagged_tokens if tag in allowed_tags and word.lower() not in stopwords]
    return ' '.join(important_words)

final_df['text'] = final_df['text'].apply(filter_important_words)

final_df['text'][0]

"""Create vectorizer"""

vectorizer = TfidfVectorizer()
X1 = vectorizer.fit_transform(data['text'])

#vectorizer.vocabulary_

def get_top_similar_products(query, num_recommendations=5):
    query = ' '.join([word.lower() for word in word_tokenize(query) if word.isalpha()])
    query = ' '.join([word for word in word_tokenize(query) if not word in stopwords])
    query_vector = vectorizer.transform([query])
    cosine_similarities = cosine_similarity(query_vector, vectorizer.transform(final_df['text'])).flatten()
    related_docs_indices = cosine_similarities.argsort()[:-num_recommendations-1:-1]
    return data.iloc[related_docs_indices].applymap(lambda x: float(x) if isinstance(x, float) else x)

# get_top_similar_products('I want black school shoes for boy', num_recommendations=3)

